{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWJYZRzuwXKw",
        "outputId": "611bdea0-0df5-4081-d821-a0f88674f431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train dataset length:  11314\n",
            "train dataset length:  7532\n"
          ]
        }
      ],
      "source": [
        "### load data set and preprocessing\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "newsgroups_test = fetch_20newsgroups(subset='test')\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to preprocess\n",
        "def lemmatizing(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token not in string.punctuation and token not in stop_words]\n",
        "    lemmatized_text = ' '.join([WordNetLemmatizer().lemmatize(token) for token in tokens])\n",
        "    return lemmatized_text\n",
        "\n",
        "def stemming(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token not in string.punctuation and token not in stop_words]\n",
        "    stemmed_tokens = ' '.join([PorterStemmer().stem(token) for token in tokens])\n",
        "    return stemmed_tokens\n",
        "\n",
        "newsgroups_train_lemmatized = [lemmatizing(x) for x in newsgroups_train.data]\n",
        "newsgroups_test_lemmatized  = [lemmatizing(x) for x in newsgroups_test.data]\n",
        "\n",
        "newsgroups_train_stemmed = [stemming(x) for x in newsgroups_train.data]\n",
        "newsgroups_test_stemmed  = [stemming(x) for x in newsgroups_test.data]\n",
        "\n",
        "print('train dataset length: ', len(newsgroups_train.data))\n",
        "print('train dataset length: ', len(newsgroups_test.data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Naive Bayes, LogR\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer,CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline,Pipeline\n",
        "from sklearn.model_selection import  GridSearchCV\n",
        "from sklearn import metrics\n",
        "import time\n",
        "\n",
        "# train,test = newsgroups_train.data , newsgroups_test.data\n",
        "# train,test = newsgroups_train_lemmatized , newsgroups_test_lemmatized\n",
        "train,test = newsgroups_train_stemmed    , newsgroups_test_stemmed\n",
        "\n",
        "\n",
        "def my_grid_search(input_pipeline, input_parameters, train,test):\n",
        "    print('---------------------------------------')\n",
        "    print( input_pipeline)\n",
        "    print( input_parameters)\n",
        "    tt=time.time()\n",
        "    grid_search = GridSearchCV(input_pipeline, input_parameters, cv=5, n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(train, newsgroups_train.target)\n",
        "    print(\"Best parameters set found on development set:\",grid_search.best_params_)\n",
        "    print(\"Grid scores on development set:\")\n",
        "    means = grid_search.cv_results_['mean_test_score']\n",
        "    stds = grid_search.cv_results_['std_test_score']\n",
        "    for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
        "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
        "    predicted = grid_search.predict(test)\n",
        "    print(metrics.classification_report(newsgroups_test.target, predicted, target_names=newsgroups_test.target_names))\n",
        "    print((time.time()-tt)/60)\n",
        "    print('---------------------------------------')\n",
        "\n",
        "\n",
        "\n",
        "for clf in [ MultinomialNB(),  LogisticRegression(max_iter=1000)]:\n",
        "    pipeline = Pipeline([\n",
        "      ('vectorizer', CountVectorizer()),\n",
        "      ('classifier', clf)\n",
        "    ])\n",
        "    parameters ={\n",
        "      'vectorizer__max_features' : [10000],\n",
        "      'vectorizer__ngram_range' : [(1, 1),(1,2),(1,3)],\n",
        "      #'transform__norm':['l1','l2']\n",
        "    }\n",
        "    my_grid_search(pipeline,parameters,train,test)\n",
        "\n",
        "for clf in [   LogisticRegression(max_iter=1000)]:#MultinomialNB()]:#  ,\n",
        "    pipeline = Pipeline([\n",
        "      ('vectorizer', CountVectorizer()),\n",
        "      ('transform', TfidfTransformer()),\n",
        "      ('classifier', clf)\n",
        "    ])\n",
        "    parameters ={\n",
        "      'vectorizer__max_features' : [10000],\n",
        "      'vectorizer__ngram_range' : [(1, 1),(1,2),(1,3)],\n",
        "      'transform__norm':['l1','l2']\n",
        "    }\n",
        "    my_grid_search(pipeline,parameters,train,test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFmaRrBHZ_sQ",
        "outputId": "d7fe66f7-fc50-4aa7-87e9-93aecb6c7d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
            "                ('classifier', MultinomialNB())])\n",
            "{'vectorizer__max_features': [10000], 'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)]}\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
            "Best parameters set found on development set: {'vectorizer__max_features': 10000, 'vectorizer__ngram_range': (1, 1)}\n",
            "Grid scores on development set:\n",
            "0.843 (+/-0.009) for {'vectorizer__max_features': 10000, 'vectorizer__ngram_range': (1, 1)}\n",
            "0.822 (+/-0.008) for {'vectorizer__max_features': 10000, 'vectorizer__ngram_range': (1, 2)}\n",
            "0.811 (+/-0.008) for {'vectorizer__max_features': 10000, 'vectorizer__ngram_range': (1, 3)}\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.75      0.81      0.78       319\n",
            "           comp.graphics       0.56      0.79      0.65       389\n",
            " comp.os.ms-windows.misc       0.20      0.00      0.01       394\n",
            "comp.sys.ibm.pc.hardware       0.51      0.72      0.60       392\n",
            "   comp.sys.mac.hardware       0.68      0.86      0.76       385\n",
            "          comp.windows.x       0.77      0.74      0.76       395\n",
            "            misc.forsale       0.77      0.81      0.79       390\n",
            "               rec.autos       0.83      0.89      0.86       396\n",
            "         rec.motorcycles       0.87      0.94      0.90       398\n",
            "      rec.sport.baseball       0.92      0.92      0.92       397\n",
            "        rec.sport.hockey       0.95      0.94      0.95       399\n",
            "               sci.crypt       0.90      0.88      0.89       396\n",
            "         sci.electronics       0.75      0.71      0.73       393\n",
            "                 sci.med       0.92      0.80      0.86       396\n",
            "               sci.space       0.88      0.89      0.88       394\n",
            "  soc.religion.christian       0.89      0.90      0.90       398\n",
            "      talk.politics.guns       0.73      0.90      0.81       364\n",
            "   talk.politics.mideast       0.97      0.84      0.90       376\n",
            "      talk.politics.misc       0.72      0.59      0.65       310\n",
            "      talk.religion.misc       0.65      0.59      0.62       251\n",
            "\n",
            "                accuracy                           0.78      7532\n",
            "               macro avg       0.76      0.78      0.76      7532\n",
            "            weighted avg       0.76      0.78      0.76      7532\n",
            "\n",
            "3.2030041893323262\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
            "                ('classifier', LogisticRegression(max_iter=1000))])\n",
            "{'vectorizer__max_features': [10000], 'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)]}\n",
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
            "Best parameters set found on development set: {'vectorizer__max_features': 10000, 'vectorizer__ngram_range': (1, 1)}\n",
            "Grid scores on development set:\n",
            "0.874 (+/-0.008) for {'vectorizer__max_features': 10000, 'vectorizer__ngram_range': (1, 1)}\n",
            "0.871 (+/-0.016) for {'vectorizer__max_features': 10000, 'vectorizer__ngram_range': (1, 2)}\n",
            "0.869 (+/-0.016) for {'vectorizer__max_features': 10000, 'vectorizer__ngram_range': (1, 3)}\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.74      0.70      0.72       319\n",
            "           comp.graphics       0.67      0.73      0.70       389\n",
            " comp.os.ms-windows.misc       0.73      0.65      0.69       394\n",
            "comp.sys.ibm.pc.hardware       0.65      0.70      0.67       392\n",
            "   comp.sys.mac.hardware       0.74      0.80      0.77       385\n",
            "          comp.windows.x       0.78      0.71      0.75       395\n",
            "            misc.forsale       0.76      0.85      0.80       390\n",
            "               rec.autos       0.85      0.84      0.85       396\n",
            "         rec.motorcycles       0.91      0.90      0.91       398\n",
            "      rec.sport.baseball       0.87      0.89      0.88       397\n",
            "        rec.sport.hockey       0.91      0.94      0.92       399\n",
            "               sci.crypt       0.92      0.87      0.90       396\n",
            "         sci.electronics       0.65      0.70      0.67       393\n",
            "                 sci.med       0.83      0.78      0.81       396\n",
            "               sci.space       0.90      0.87      0.88       394\n",
            "  soc.religion.christian       0.83      0.88      0.85       398\n",
            "      talk.politics.guns       0.69      0.83      0.76       364\n",
            "   talk.politics.mideast       0.96      0.78      0.86       376\n",
            "      talk.politics.misc       0.70      0.54      0.61       310\n",
            "      talk.religion.misc       0.55      0.58      0.57       251\n",
            "\n",
            "                accuracy                           0.78      7532\n",
            "               macro avg       0.78      0.78      0.78      7532\n",
            "            weighted avg       0.79      0.78      0.78      7532\n",
            "\n",
            "8.899460132916769\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
            "                ('transform', TfidfTransformer()),\n",
            "                ('classifier', LogisticRegression(max_iter=1000))])\n",
            "{'vectorizer__max_features': [10000], 'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)], 'transform__norm': ['l1', 'l2']}\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "Best parameters set found on development set: {'transform__norm': 'l2', 'vectorizer__max_features': 10000, 'vectorizer__ngram_range': (1, 1)}\n",
            "Grid scores on development set:\n",
            "0.760 (+/-0.011) for {'transform__norm': 'l1', 'vectorizer__max_features': 10000, 'vectorizer__ngram_range': (1, 1)}\n",
            "0.741 (+/-0.013) for {'transform__norm': 'l1', 'vectorizer__max_features': 10000, 'vectorizer__ngram_range': (1, 2)}\n",
            "0.736 (+/-0.014) for {'transform__norm': 'l1', 'vectorizer__max_features': 10000, 'vectorizer__ngram_range': (1, 3)}\n",
            "0.888 (+/-0.010) for {'transform__norm': 'l2', 'vectorizer__max_features': 10000, 'vectorizer__ngram_range': (1, 1)}\n",
            "0.880 (+/-0.010) for {'transform__norm': 'l2', 'vectorizer__max_features': 10000, 'vectorizer__ngram_range': (1, 2)}\n",
            "0.877 (+/-0.011) for {'transform__norm': 'l2', 'vectorizer__max_features': 10000, 'vectorizer__ngram_range': (1, 3)}\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.76      0.71      0.73       319\n",
            "           comp.graphics       0.69      0.78      0.73       389\n",
            " comp.os.ms-windows.misc       0.73      0.71      0.72       394\n",
            "comp.sys.ibm.pc.hardware       0.70      0.71      0.71       392\n",
            "   comp.sys.mac.hardware       0.81      0.83      0.82       385\n",
            "          comp.windows.x       0.83      0.75      0.79       395\n",
            "            misc.forsale       0.76      0.85      0.80       390\n",
            "               rec.autos       0.90      0.88      0.89       396\n",
            "         rec.motorcycles       0.96      0.93      0.95       398\n",
            "      rec.sport.baseball       0.88      0.92      0.90       397\n",
            "        rec.sport.hockey       0.94      0.96      0.95       399\n",
            "               sci.crypt       0.94      0.89      0.92       396\n",
            "         sci.electronics       0.70      0.77      0.73       393\n",
            "                 sci.med       0.88      0.85      0.86       396\n",
            "               sci.space       0.89      0.91      0.90       394\n",
            "  soc.religion.christian       0.81      0.91      0.86       398\n",
            "      talk.politics.guns       0.73      0.90      0.80       364\n",
            "   talk.politics.mideast       0.96      0.87      0.92       376\n",
            "      talk.politics.misc       0.80      0.60      0.69       310\n",
            "      talk.religion.misc       0.71      0.49      0.58       251\n",
            "\n",
            "                accuracy                           0.82      7532\n",
            "               macro avg       0.82      0.81      0.81      7532\n",
            "            weighted avg       0.82      0.82      0.82      7532\n",
            "\n",
            "8.625378914674123\n",
            "---------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Glove Embedding\n",
        "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer,CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline,Pipeline\n",
        "from sklearn.model_selection import  GridSearchCV\n",
        "from sklearn import metrics\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# train,test = newsgroups_train.data , newsgroups_test.data\n",
        "# train,test = newsgroups_train_lemmatized , newsgroups_test_lemmatized\n",
        "train,test = newsgroups_train_stemmed    , newsgroups_test_stemmed\n",
        "\n",
        "\n",
        "\n",
        "glove_file = 'glove.6B.100d.txt'  # Adjust the path accordingly\n",
        "word_vectors = {}\n",
        "with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "          values = line.split()\n",
        "          word = values[0]\n",
        "          vector = np.array(values[1:], dtype='float32')\n",
        "          word_vectors[word] = vector\n",
        "        except:\n",
        "            print(line)\n",
        "            pass\n",
        "\n",
        "\n",
        "X_train_glove , X_test_glove = [] , []\n",
        "vectorizer = CountVectorizer(max_features=100000)\n",
        "vectorizer.fit_transform(train)\n",
        "vectorizer.transform(test)\n",
        "\n",
        "word_index = vectorizer.vocabulary_\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 100))  # Adjust dimensions based on GloVe file used\n",
        "for word, i in word_index.items():\n",
        "    if word in word_vectors:\n",
        "        embedding_matrix[i] = word_vectors[word]\n",
        "\n",
        "for doc in newsgroups_train.data:\n",
        "    words = doc.split()\n",
        "    embeddings = [embedding_matrix[word_index[word]] for word in words if word in word_index]\n",
        "    if embeddings: X_train_glove.append(np.mean(embeddings, axis=0))\n",
        "    else: X_train_glove.append(np.zeros(100))  # Zero vector if no embeddings found\n",
        "\n",
        "for doc in newsgroups_test.data:\n",
        "    words = doc.split()\n",
        "    embeddings = [embedding_matrix[word_index[word]] for word in words if word in word_index]\n",
        "    if embeddings: X_test_glove.append(np.mean(embeddings, axis=0))\n",
        "    else: X_test_glove.append(np.zeros(100))  # Zero vector if no embeddings found\n",
        "\n",
        "X_train_glove = np.array(X_train_glove)\n",
        "X_test_glove = np.array(X_test_glove)\n",
        "\n",
        "for clf in [  LogisticRegression(max_iter=1000)]:\n",
        "    pipeline = Pipeline([\n",
        "      ('classifier', clf)  #   LogisticRegression(max_iter=1000)\n",
        "    ])\n",
        "    tt=time.time()\n",
        "    pipeline.fit(X_train_glove , newsgroups_train.target)\n",
        "    predicted = pipeline.predict(X_test_glove)\n",
        "    print(metrics.classification_report(newsgroups_test.target, predicted, target_names=newsgroups_test.target_names))\n",
        "    print((time.time()-tt)/60)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXwuTCAXwERe",
        "outputId": "60825fd5-ffa2-4c0c-8056-162a67b60ead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.20      0.07      0.10       319\n",
            "           comp.graphics       0.42      0.36      0.39       389\n",
            " comp.os.ms-windows.misc       0.34      0.18      0.23       394\n",
            "comp.sys.ibm.pc.hardware       0.41      0.37      0.39       392\n",
            "   comp.sys.mac.hardware       0.35      0.07      0.12       385\n",
            "          comp.windows.x       0.40      0.46      0.43       395\n",
            "            misc.forsale       0.54      0.71      0.61       390\n",
            "               rec.autos       0.54      0.57      0.55       396\n",
            "         rec.motorcycles       0.41      0.58      0.48       398\n",
            "      rec.sport.baseball       0.46      0.55      0.50       397\n",
            "        rec.sport.hockey       0.60      0.63      0.62       399\n",
            "               sci.crypt       0.48      0.55      0.51       396\n",
            "         sci.electronics       0.50      0.39      0.44       393\n",
            "                 sci.med       0.54      0.66      0.59       396\n",
            "               sci.space       0.55      0.59      0.57       394\n",
            "  soc.religion.christian       0.27      0.76      0.40       398\n",
            "      talk.politics.guns       0.44      0.35      0.39       364\n",
            "   talk.politics.mideast       0.42      0.60      0.49       376\n",
            "      talk.politics.misc       0.54      0.05      0.09       310\n",
            "      talk.religion.misc       0.00      0.00      0.00       251\n",
            "\n",
            "                accuracy                           0.44      7532\n",
            "               macro avg       0.42      0.42      0.40      7532\n",
            "            weighted avg       0.43      0.44      0.41      7532\n",
            "\n",
            "0.07844879627227783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.datasets import fetch_20newsgroups\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# from gensim.models import Word2Vec\n",
        "# from sklearn import metrics\n",
        "# import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# train,test = newsgroups_train.data , newsgroups_test.data\n",
        "# # train,test = newsgroups_train_lemmatized , newsgroups_test_lemmatized\n",
        "# # train,test = newsgroups_train_stemmed    , newsgroups_test_stemmed\n",
        "\n",
        "\n",
        "# # Tokenize the text and train Word2Vec embeddings\n",
        "# tokenized_train_text = [doc.split() for doc in train]\n",
        "# word2vec_model = Word2Vec(sentences=tokenized_train_text, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# class Word2VecTransformer:\n",
        "#     def __init__(self, word2vec):\n",
        "#         self.word2vec = word2vec\n",
        "#         self.vector_size = word2vec.vector_size\n",
        "#     def fit(self, X, y=None):\n",
        "#         return self\n",
        "#     def transform(self, X):\n",
        "#         return np.array([\n",
        "#             np.mean([self.word2vec.wv[word] for word in doc.split() if word in self.word2vec.wv] or [np.zeros(self.vector_size)], axis=0)\n",
        "#             for doc in X\n",
        "#         ])\n",
        "\n",
        "# # Build the pipeline\n",
        "# pipeline = Pipeline([\n",
        "#     ('word2vec', Word2VecTransformer(word2vec_model)),\n",
        "#     ('classifier', LogisticRegression(max_iter=1000))\n",
        "# ])\n",
        "\n",
        "# # Train the model\n",
        "# pipeline.fit()\n",
        "# predicted = pipeline.predict(newsgroups_test.data)\n",
        "# print(metrics.classification_report(newsgroups_test.target, predicted, target_names=newsgroups_test.target_names))\n"
      ],
      "metadata": {
        "id": "jUJX8iv13pbU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b1bf113-88bd-42e3-a55b-55ff114f1104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.27      0.33      0.30       319\n",
            "           comp.graphics       0.29      0.31      0.30       389\n",
            " comp.os.ms-windows.misc       0.33      0.22      0.26       394\n",
            "comp.sys.ibm.pc.hardware       0.31      0.39      0.35       392\n",
            "   comp.sys.mac.hardware       0.26      0.24      0.25       385\n",
            "          comp.windows.x       0.47      0.44      0.45       395\n",
            "            misc.forsale       0.69      0.70      0.70       390\n",
            "               rec.autos       0.27      0.32      0.29       396\n",
            "         rec.motorcycles       0.44      0.39      0.42       398\n",
            "      rec.sport.baseball       0.31      0.29      0.30       397\n",
            "        rec.sport.hockey       0.51      0.50      0.50       399\n",
            "               sci.crypt       0.36      0.50      0.41       396\n",
            "         sci.electronics       0.30      0.19      0.23       393\n",
            "                 sci.med       0.29      0.27      0.28       396\n",
            "               sci.space       0.34      0.30      0.32       394\n",
            "  soc.religion.christian       0.42      0.66      0.51       398\n",
            "      talk.politics.guns       0.31      0.40      0.35       364\n",
            "   talk.politics.mideast       0.47      0.52      0.49       376\n",
            "      talk.politics.misc       0.28      0.15      0.19       310\n",
            "      talk.religion.misc       0.21      0.08      0.11       251\n",
            "\n",
            "                accuracy                           0.37      7532\n",
            "               macro avg       0.36      0.36      0.35      7532\n",
            "            weighted avg       0.36      0.37      0.36      7532\n",
            "\n"
          ]
        }
      ]
    }
  ]
}