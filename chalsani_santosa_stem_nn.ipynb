{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjRWtJ6zYQJ-"
      },
      "outputs": [],
      "source": [
        "# import shutil and nltk library\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from nltk import download, word_tokenize, pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download 20news-bydate.tar.gz file\n",
        "\n",
        "!curl 'http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz' >> './20news-bydate.tar.gz'"
      ],
      "metadata": {
        "id": "PZlaS1Nffc_H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "363e2bd3-96c4-4c18-dcd1-2cff47d3a14a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 13.7M  100 13.7M    0     0  2131k      0  0:00:06  0:00:06 --:--:-- 2948k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unpack files\n",
        "\n",
        "shutil.unpack_archive('/content/20news-bydate.tar.gz', '/content/20news-bydate')\n",
        "!rm '/content/20news-bydate.tar.gz' # delete file"
      ],
      "metadata": {
        "id": "a2W8HbvdgbmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources if not already downloaded\n",
        "download('punkt')\n",
        "download('averaged_perceptron_tagger')\n",
        "download('wordnet')\n",
        "\n",
        "# Function to perform lemmatization\n",
        "def stem_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = []\n",
        "\n",
        "    # Perform POS tagging for lemmatization\n",
        "    pos_tags = pos_tag(word_tokenize(text))\n",
        "\n",
        "    for word, pos in pos_tags:\n",
        "        # Map POS tags to WordNet POS tags\n",
        "        wn_pos = pos[0].lower() if pos[0].lower() in 'nvar' else None\n",
        "\n",
        "        if wn_pos:\n",
        "            stemmed_word = stemmer.stem(word, wn_pos)\n",
        "        else:\n",
        "            stemmed_word = stemmer.stem(word)\n",
        "\n",
        "        stemmed_tokens.append(stemmed_word)\n",
        "\n",
        "    return ' '.join(stemmed_tokens)\n",
        "\n",
        "# Function to lemmatize all documents in a directory\n",
        "def stem_directory(directory):\n",
        "  for root, dirs, files in os.walk(directory):\n",
        "    print(f'Processing {root}... found directories: {dirs} and {len(files)} files...')\n",
        "    for file in files:\n",
        "        file_path = os.path.join(root, file)\n",
        "\n",
        "        # read file and get lematized content\n",
        "        with open(file_path, 'r', encoding='latin1') as f:\n",
        "            content = f.read()\n",
        "        stemmed_content = stem_text(content)\n",
        "\n",
        "        # write lematized content to new file\n",
        "        with open(file_path, 'w', encoding='latin1') as f:\n",
        "            f.write(stemmed_content)\n",
        "    print(f'Finished {root}.')\n",
        "  print('Finished lemmatizing all files.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQEePrAHYeDY",
        "outputId": "6a3ef413-64f6-4eb4-dfb8-7fba76c1881f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/20news-bydate'\n",
        "#stem_directory(data_path)"
      ],
      "metadata": {
        "id": "P0d9JtL0ZyXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import sklearn/tensorflow and other dependencies\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "dZ2I2z1Hh79K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data(data_path):\n",
        "  texts = []\n",
        "  labels = []\n",
        "\n",
        "  for root, dirs, files in os.walk(data_path):\n",
        "    for file in files:\n",
        "      file_path = os.path.join(root, file)\n",
        "\n",
        "      # read file and append content and label\n",
        "      with open(file_path, 'r', encoding='latin1') as f:\n",
        "        content = f.read()\n",
        "        texts.append(content)\n",
        "        labels.append(root.split(\"/\")[-1])\n",
        "\n",
        "  # Create a DataFrame for better handling\n",
        "  df = pd.DataFrame({'text': texts, 'label': labels})\n",
        "\n",
        "  # Encode labels\n",
        "  label_encoder = LabelEncoder()\n",
        "  df['encoded_label'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "  # Split the data into training and testing sets\n",
        "  train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    df['text'], df['encoded_label'], test_size=0.2, random_state=42\n",
        "  )\n",
        "\n",
        "  return train_data, test_data, train_labels, test_labels, label_encoder, texts, df['encoded_label'].tolist()"
      ],
      "metadata": {
        "id": "XmNZWjvJkAvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the lemmatized 20 Newsgroups dataset\n",
        "train_data, test_data, train_labels, test_labels, label_encoder, texts, labels = load_and_preprocess_data(data_path)\n",
        "\n",
        "# Tokenize and pad the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_length = 200  # Adjust this based on your dataset and available resources\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_data)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
        "\n",
        "X_train = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
        "X_test = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Create a simple neural network model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 32, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, train_labels, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, test_labels)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sDM1ACvkO0E",
        "outputId": "b5f1a0b2-48c6-4ceb-e4a9-9fd7fb0c2cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "377/377 [==============================] - 47s 118ms/step - loss: 2.4570 - accuracy: 0.2483 - val_loss: 1.4757 - val_accuracy: 0.5454\n",
            "Epoch 2/50\n",
            "377/377 [==============================] - 19s 51ms/step - loss: 0.5588 - accuracy: 0.8797 - val_loss: 0.7657 - val_accuracy: 0.7759\n",
            "Epoch 3/50\n",
            "377/377 [==============================] - 13s 34ms/step - loss: 0.0572 - accuracy: 0.9945 - val_loss: 0.7090 - val_accuracy: 0.7818\n",
            "Epoch 4/50\n",
            "377/377 [==============================] - 8s 22ms/step - loss: 0.0158 - accuracy: 0.9993 - val_loss: 0.6927 - val_accuracy: 0.7974\n",
            "Epoch 5/50\n",
            "377/377 [==============================] - 6s 16ms/step - loss: 0.0085 - accuracy: 0.9993 - val_loss: 0.6809 - val_accuracy: 0.8007\n",
            "Epoch 6/50\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.0059 - accuracy: 0.9994 - val_loss: 0.6764 - val_accuracy: 0.8031\n",
            "Epoch 7/50\n",
            "377/377 [==============================] - 5s 14ms/step - loss: 0.0045 - accuracy: 0.9994 - val_loss: 0.6852 - val_accuracy: 0.8037\n",
            "Epoch 8/50\n",
            "377/377 [==============================] - 5s 12ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.6772 - val_accuracy: 0.8057\n",
            "Epoch 9/50\n",
            "377/377 [==============================] - 5s 13ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.6788 - val_accuracy: 0.8054\n",
            "Epoch 10/50\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.6942 - val_accuracy: 0.8074\n",
            "Epoch 11/50\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.6827 - val_accuracy: 0.8080\n",
            "Epoch 12/50\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.7037 - val_accuracy: 0.8080\n",
            "Epoch 13/50\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.6800 - val_accuracy: 0.8130\n",
            "Epoch 14/50\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.7219 - val_accuracy: 0.8040\n",
            "Epoch 15/50\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.7040 - val_accuracy: 0.8060\n",
            "Epoch 16/50\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.0011 - accuracy: 0.9994 - val_loss: 0.6928 - val_accuracy: 0.8150\n",
            "Epoch 17/50\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 8.1587e-04 - accuracy: 0.9996 - val_loss: 0.7009 - val_accuracy: 0.8130\n",
            "Epoch 18/50\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 6.2456e-04 - accuracy: 0.9998 - val_loss: 0.7059 - val_accuracy: 0.8120\n",
            "Epoch 19/50\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 4.7286e-04 - accuracy: 0.9998 - val_loss: 0.7117 - val_accuracy: 0.8133\n",
            "Epoch 20/50\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 5.1514e-04 - accuracy: 0.9997 - val_loss: 0.7139 - val_accuracy: 0.8127\n",
            "Epoch 21/50\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 4.1463e-04 - accuracy: 0.9998 - val_loss: 0.7216 - val_accuracy: 0.8137\n",
            "Epoch 22/50\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 7.4095e-04 - accuracy: 0.9998 - val_loss: 0.7250 - val_accuracy: 0.8153\n",
            "Epoch 23/50\n",
            "377/377 [==============================] - 2s 5ms/step - loss: 6.2457e-04 - accuracy: 0.9997 - val_loss: 0.7199 - val_accuracy: 0.8163\n",
            "Epoch 24/50\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 4.1245e-04 - accuracy: 0.9998 - val_loss: 0.7247 - val_accuracy: 0.8173\n",
            "Epoch 25/50\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 3.9218e-04 - accuracy: 0.9998 - val_loss: 0.7296 - val_accuracy: 0.8156\n",
            "Epoch 26/50\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 3.6955e-04 - accuracy: 0.9998 - val_loss: 0.7347 - val_accuracy: 0.8163\n",
            "Epoch 27/50\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 3.7536e-04 - accuracy: 0.9998 - val_loss: 0.7428 - val_accuracy: 0.8160\n",
            "Epoch 28/50\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 3.3104e-04 - accuracy: 0.9997 - val_loss: 0.7451 - val_accuracy: 0.8166\n",
            "Epoch 29/50\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 3.4066e-04 - accuracy: 0.9998 - val_loss: 0.7507 - val_accuracy: 0.8173\n",
            "Epoch 30/50\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 3.5903e-04 - accuracy: 0.9998 - val_loss: 0.7572 - val_accuracy: 0.8196\n",
            "Epoch 31/50\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.7536 - val_accuracy: 0.8233\n",
            "Epoch 32/50\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 6.9813e-04 - accuracy: 0.9997 - val_loss: 0.7704 - val_accuracy: 0.8153\n",
            "Epoch 33/50\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 4.6619e-04 - accuracy: 0.9998 - val_loss: 0.7800 - val_accuracy: 0.8196\n",
            "Epoch 34/50\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 6.1373e-04 - accuracy: 0.9996 - val_loss: 0.7642 - val_accuracy: 0.8206\n",
            "Epoch 35/50\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 3.6759e-04 - accuracy: 0.9998 - val_loss: 0.7730 - val_accuracy: 0.8213\n",
            "Epoch 36/50\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 3.8391e-04 - accuracy: 0.9997 - val_loss: 0.7745 - val_accuracy: 0.8206\n",
            "Epoch 37/50\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 3.3975e-04 - accuracy: 0.9997 - val_loss: 0.7816 - val_accuracy: 0.8266\n",
            "Epoch 38/50\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 3.5975e-04 - accuracy: 0.9998 - val_loss: 0.7860 - val_accuracy: 0.8246\n",
            "Epoch 39/50\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 3.4418e-04 - accuracy: 0.9998 - val_loss: 0.7997 - val_accuracy: 0.8259\n",
            "Epoch 40/50\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 3.6960e-04 - accuracy: 0.9998 - val_loss: 0.7967 - val_accuracy: 0.8253\n",
            "Epoch 41/50\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 2.9173e-04 - accuracy: 0.9997 - val_loss: 0.8024 - val_accuracy: 0.8246\n",
            "Epoch 42/50\n",
            "377/377 [==============================] - 2s 5ms/step - loss: 4.8744e-04 - accuracy: 0.9998 - val_loss: 0.7918 - val_accuracy: 0.8236\n",
            "Epoch 43/50\n",
            "377/377 [==============================] - 2s 5ms/step - loss: 3.2589e-04 - accuracy: 0.9997 - val_loss: 0.7953 - val_accuracy: 0.8239\n",
            "Epoch 44/50\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 3.2840e-04 - accuracy: 0.9998 - val_loss: 0.8013 - val_accuracy: 0.8266\n",
            "Epoch 45/50\n",
            "377/377 [==============================] - 2s 5ms/step - loss: 4.3396e-04 - accuracy: 0.9998 - val_loss: 0.8347 - val_accuracy: 0.8223\n",
            "Epoch 46/50\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 6.9215e-04 - accuracy: 0.9998 - val_loss: 0.8218 - val_accuracy: 0.8190\n",
            "Epoch 47/50\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 3.2069e-04 - accuracy: 0.9998 - val_loss: 0.8160 - val_accuracy: 0.8223\n",
            "Epoch 48/50\n",
            "377/377 [==============================] - 2s 5ms/step - loss: 3.0534e-04 - accuracy: 0.9998 - val_loss: 0.8232 - val_accuracy: 0.8229\n",
            "Epoch 49/50\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 3.1996e-04 - accuracy: 0.9998 - val_loss: 0.8356 - val_accuracy: 0.8246\n",
            "Epoch 50/50\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 3.8380e-04 - accuracy: 0.9998 - val_loss: 0.8318 - val_accuracy: 0.8253\n",
            "118/118 [==============================] - 0s 4ms/step - loss: 0.8524 - accuracy: 0.8162\n",
            "Test Accuracy: 81.62%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "sO57PiJmki7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
        "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}"
      ],
      "metadata": {
        "id": "z4062Eb4pdXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_classes):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        x = self.dropout(pooled_output)\n",
        "        logits = self.fc(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "kgbklnXqqiEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()"
      ],
      "metadata": {
        "id": "utKqF2BtqlEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actual_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            predictions.extend(preds.cpu().tolist())\n",
        "            actual_labels.extend(labels.cpu().tolist())\n",
        "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)"
      ],
      "metadata": {
        "id": "71jUV-ccqnyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text, model, tokenizer, device, max_length=128):\n",
        "    model.eval()\n",
        "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "    return \"positive\" if preds.item() == 1 else \"negative\""
      ],
      "metadata": {
        "id": "CFDtvaUYqwpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model_name = 'bert-base-uncased'\n",
        "num_classes = 2\n",
        "max_length = 128\n",
        "batch_size = 16\n",
        "num_epochs = 4\n",
        "learning_rate = 2e-5"
      ],
      "metadata": {
        "id": "7B23i90Gq4uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)\n",
        "val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "8F_4QRmcq69z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BERTClassifier(bert_model_name, num_classes).to(device)"
      ],
      "metadata": {
        "id": "tpSy2k6rrB3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhvnAcxArVmp",
        "outputId": "2c551fb8-1af9-45ae-a1af-ef7def9c836f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        train(model, train_dataloader, optimizer, scheduler, device)\n",
        "        accuracy, report = evaluate(model, val_dataloader, device)\n",
        "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "        print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "OxAgq9YSrY6V",
        "outputId": "fbb0034d-97a0-4440-8d2b-c597791c460b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-c676275e3021>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1}/{num_epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Validation Accuracy: {accuracy:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-5caa499cf658>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, optimizer, scheduler, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    125\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 new_grads.append(\n\u001b[0;32m--> 127\u001b[0;31m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 )\n\u001b[1;32m    129\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TMfO0fdarbtE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}